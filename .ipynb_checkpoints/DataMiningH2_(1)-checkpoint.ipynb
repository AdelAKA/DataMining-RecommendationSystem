{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btaAUKDMvN8X"
   },
   "source": [
    "# Download the data\n",
    " We start by Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIau-pdlyIXR",
    "outputId": "2d829d61-758a-4688-dfa2-d9a2b807ae75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGC4on8izM51"
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/goodreads_reviews_spoiler.json.gz\" -r \"drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0vdLw7h1uQr",
    "outputId": "bab42496-f0e1-4bab-b760-ece12c043d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading https://files.pythonhosted.org/packages/52/b9/d426f164f35bb50d512a77d6a7c5eb70b2bea3459dc10f73f130ba732810/gdown-3.13.0.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests[socks]>=2.12.0 in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gdown: filename=gdown-3.13.0-cp37-none-any.whl size=9046 sha256=d0a03f2d75db1be5f9ac5432a04db60b6db4635302731364398ccc49fb234ece\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4n27rqbb/wheels/ba/fa/c5/12813d7496f34652c43a471e11a780e769889d06e34735c32e\n",
      "Successfully built gdown\n",
      "Installing collected packages: gdown\n",
      "  Found existing installation: gdown 3.6.4\n",
      "    Uninstalling gdown-3.6.4:\n",
      "      Successfully uninstalled gdown-3.6.4\n",
      "Successfully installed gdown-3.13.0\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=196W2kDoZXRPjzbTjM6uvTidn6aTpsFnS\n",
      "To: /content/goodreads_reviews_spoiler.json.gz\n",
      "620MB [00:05, 121MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%shell\n",
    "# pip install -U --no-cache-dir gdown --pre\n",
    "# gdown https://drive.google.com/uc?id=196W2kDoZXRPjzbTjM6uvTidn6aTpsFnS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gxewmw70a8Tq",
    "outputId": "1010435c-df35-412b-85b3-623ebd9cae19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8MB 348kB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1617596 sha256=a1112ce368c39bc6640cb9405467fe8e55b175403d4313d12d07b6d4f07ae24c\n",
      "  Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6Uar_WGaBEUF"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igs2lyWrGLYX"
   },
   "source": [
    "## Now we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iqXndbKfXhZ7"
   },
   "outputs": [],
   "source": [
    "user_ids = []\n",
    "book_ids = []\n",
    "rating = []\n",
    "chunks = pd.read_json('/content/drive/MyDrive/goodreads_reviews_spoiler.json.gz', lines=True, chunksize = 100)\n",
    "for chunk in chunks:\n",
    "    user_ids.extend(chunk.iloc[:,0])\n",
    "    rating.extend(chunk.iloc[:,3])\n",
    "    book_ids.extend(chunk.iloc[:,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KGN-ug8izvkM"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(user_ids,book_ids,rating)), columns=['user_id','book_id','rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXW3moC6Gdfk"
   },
   "source": [
    "\n",
    "We free the RAM by cleaning unneeded objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QQyrEpV84Kpk"
   },
   "outputs": [],
   "source": [
    "chunks = None\n",
    "user_ids = None\n",
    "book_ids = None\n",
    "rating = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATVYd1QiGm2y"
   },
   "source": [
    "## Collaborative Filtering\n",
    "In our first test we will use collaborative filtering to get the similarity matrix between the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IwGyYLTfz0Ea"
   },
   "outputs": [],
   "source": [
    "testdf=df[['user_id','rating']].groupby(df['book_id'])\n",
    "listOfDictonaries=[]\n",
    "indexMap = {}\n",
    "reverseIndexMap = {}\n",
    "ptr=0;\n",
    "# go through the data and store in dictionary\n",
    "for groupKey in testdf.groups.keys():\n",
    "    tempDict={}\n",
    "    groupDF = testdf.get_group(groupKey)\n",
    "    for i in range(0,len(groupDF)):\n",
    "        tempDict[groupDF.iloc[i,0]]=groupDF.iloc[i,1]\n",
    "    indexMap[ptr]=groupKey\n",
    "    reverseIndexMap[groupKey] = ptr\n",
    "    ptr=ptr+1\n",
    "    listOfDictonaries.append(tempDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IDGJ1Zb9z3NK"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dictVectorizer = DictVectorizer(sparse=True)\n",
    "vector = dictVectorizer.fit_transform(listOfDictonaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Rcd1WtG7tH"
   },
   "source": [
    "### Compute similarity\n",
    "Here we compute the similarity between books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iY2S-7WBz6ei"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "pairwiseSimilarity = cosine_similarity(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aVnaTAkDz8wi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getTopRecommandations(bookID):\n",
    "    row = reverseIndexMap[bookID]\n",
    "    print(\"------INPUT BOOK--------\")\n",
    "    print(\"Book-ID:\",bookID)\n",
    "    # printBookDetails(bookID)\n",
    "    print(\"-------RECOMMENDATIONS----------\")\n",
    "    similarBookIDs = [print(\"Book-ID:\",indexMap[i]) for i in np.argsort(pairwiseSimilarity[row])[-7:-2][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJfX_nqNHHAq"
   },
   "source": [
    "### Predict\n",
    "The recommendation here takes a Book and gives similar books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5ZQEHUfz-2c",
    "outputId": "c793360c-cb16-4d82-b791-9e8f17d606ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------INPUT BOOK--------\n",
      "Book-ID: 16981\n",
      "-------RECOMMENDATIONS----------\n",
      "Book-ID: 59950\n",
      "Book-ID: 53639\n",
      "Book-ID: 48467\n",
      "Book-ID: 26530351\n",
      "Book-ID: 12953517\n"
     ]
    }
   ],
   "source": [
    "getTopRecommandations(16981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3FU8Al1k2JRx"
   },
   "outputs": [],
   "source": [
    "dictVectorizer = None\n",
    "vector = None\n",
    "pairwiseSimilarity = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp3k1-ZxHSwn"
   },
   "source": [
    "## Surprise Library\n",
    "In the second test we will try the Surpise library to get our recommendatios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OENj6yFQIj_U"
   },
   "outputs": [],
   "source": [
    "# the data is too big for the library so \n",
    "# we have to take some of the data and test on it\n",
    "df2 = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c_Z055j38BqK"
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "import heapq\n",
    "\n",
    "# define the values of rating for the Surprise reader\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df2[[\"user_id\", \"book_id\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HTU9WUj58kt5"
   },
   "outputs": [],
   "source": [
    "# prepare the training set\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vmyLWB_JRtb"
   },
   "source": [
    "### KNN Modul\n",
    "Trying the Knn modul for out first test and we compute the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnTULuCm8lgr",
    "outputId": "95220fbf-cbe7-4fc5-98cb-7d18fc8d0b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/surprise/prediction_algorithms/algo_base.py:249: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim = construction_func[name](*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic\n",
    "similarity_matrix_KNN = KNNBasic(sim_options={\n",
    "        'name': 'cosine',\n",
    "        'user_based': False\n",
    "        })\\\n",
    "        .fit(trainset)\\\n",
    "        .compute_similarities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnqnH5RvJ3Xw"
   },
   "source": [
    "### SVD Modul\n",
    "Now we try the SVD and we compute the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "joeiP5nbA-el"
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "similarity_matrix_SVD = SVD().fit(trainset).compute_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Y-RQ2cWR8tRW"
   },
   "outputs": [],
   "source": [
    "# this is our test subject\n",
    "test_subject = '8842281e1d1347389f2ab93d60773d4d'\n",
    "# Get the top K items user rated\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eEAmx0oB8yy5"
   },
   "outputs": [],
   "source": [
    "# the surprise library makes a new ids for the users for\n",
    "# memory manegment, so we have to get the inner id of our test subject\n",
    "test_subject_iid = trainset.to_inner_uid(test_subject)\n",
    "\n",
    "# Get the top K items the user rated\n",
    "test_subject_ratings = trainset.ur[test_subject_iid]\n",
    "k_neighbors = heapq.nlargest(k, test_subject_ratings, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2_nSLK5LNjj"
   },
   "source": [
    "### KNN Recommendation\n",
    "here we pick the candidates from our data that are most similer to the user choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BFwdjVDf85_Z"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Default dict is basically a standard dictionary,\n",
    "# the difference beeing that it doesn't throw an error\n",
    "# when trying to access a key which does not exist,\n",
    "# instead a new entry, with that key, is created.\n",
    "candidates = defaultdict(float)\n",
    "\n",
    "for itemID, rating in k_neighbors:\n",
    "    try:\n",
    "      similaritities = similarity_matrix_KNN[itemID]\n",
    "      for innerID, score in enumerate(similaritities):\n",
    "          candidates[innerID] += score * (rating / 5.0)\n",
    "    except:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTEBfIuBLiOf"
   },
   "source": [
    "we pick the best recommendation to our user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CH2ORJAgAOUu",
    "outputId": "cccb0dcf-23af-414d-a87f-7f3d52ca5cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BookID:  4667024\n",
      "BookID:  15724396\n",
      "BookID:  7937843\n",
      "BookID:  26114135\n",
      "BookID:  28016509\n",
      "BookID:  31450633\n",
      "BookID:  18812405\n",
      "BookID:  22839846\n",
      "BookID:  22609310\n",
      "BookID:  35244824\n",
      "BookID:  31393997\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "read = {}\n",
    "for itemID, rating in trainset.ur[test_subject_iid]:\n",
    "  read[itemID] = 1\n",
    "\n",
    "# Add items to list of user's recommendations\n",
    "# If they are similar to their favorite movies,\n",
    "# AND have not already been read.\n",
    "recommendations = []\n",
    "\n",
    "position = 0\n",
    "for itemID, rating_sum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "  if not itemID in read:\n",
    "    recommendations.append(trainset.to_raw_iid(itemID))\n",
    "    position += 1\n",
    "    if (position > 10): break # We only want top 10\n",
    "\n",
    "for rec in recommendations:\n",
    "  print(\"BookID: \", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeX9HfSXL0mA"
   },
   "source": [
    "### SVD Recommendation\n",
    "now we do the same thing but with the SVD modul similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Qe5Rjh3NAO-T"
   },
   "outputs": [],
   "source": [
    "candidates = defaultdict(float)\n",
    "\n",
    "for itemID, rating in k_neighbors:\n",
    "    try:\n",
    "      similaritities = similarity_matrix_SVD[itemID]\n",
    "      for innerID, score in enumerate(similaritities):\n",
    "          candidates[innerID] += score * (rating / 5.0)\n",
    "    except:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CU5vKEGLvy6",
    "outputId": "d4b074c0-bd8a-45c6-ebbd-ef5a5c332c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BookID:  35275234\n",
      "BookID:  34705260\n",
      "BookID:  34787781\n",
      "BookID:  33807229\n",
      "BookID:  32054095\n",
      "BookID:  36065344\n",
      "BookID:  35244824\n",
      "BookID:  34776338\n",
      "BookID:  35604204\n",
      "BookID:  24044596\n",
      "BookID:  31393997\n"
     ]
    }
   ],
   "source": [
    "read = {}\n",
    "for itemID, rating in trainset.ur[test_subject_iid]:\n",
    "  read[itemID] = 1\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "position = 0\n",
    "for itemID, rating_sum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "  if not itemID in read:\n",
    "    recommendations.append(trainset.to_raw_iid(itemID))\n",
    "    position += 1\n",
    "    if (position > 10): break\n",
    "\n",
    "for rec in recommendations:\n",
    "  print(\"BookID: \", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxGOVbtQOm3q"
   },
   "source": [
    "We see that each modul did a diffirent prediction.\n",
    "And a small note: the SVD model can easily handle all the 1M set of data, but the KNN modul is the one with the RAM problem, but we used the same data on both for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gCC4j_wzNHx5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataMiningH2 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
